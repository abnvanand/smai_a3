{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2. (20 points) Question carry forwarded from assignment-2.\n",
    "Use the Admission dataset to perform the following task.  \n",
    "Dataset can be downloaded from http://preon.iiit.ac.in/~sanjoy_chowdhury/AdmissionDataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ipdb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from utils import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_orig = pd.read_csv(\"data/admission/data.csv\")\n",
    "df_orig.head()\n",
    "df_orig.drop([\"Serial No.\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>316.542222</td>\n",
       "      <td>107.162222</td>\n",
       "      <td>3.126667</td>\n",
       "      <td>3.361111</td>\n",
       "      <td>3.468889</td>\n",
       "      <td>8.577600</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.720889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.335705</td>\n",
       "      <td>6.023554</td>\n",
       "      <td>1.140254</td>\n",
       "      <td>0.993374</td>\n",
       "      <td>0.919432</td>\n",
       "      <td>0.599454</td>\n",
       "      <td>0.497701</td>\n",
       "      <td>0.141398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>290.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>308.250000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.122500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.630000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>317.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>8.560000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>325.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>340.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        GRE Score  TOEFL Score  University Rating         SOP        LOR   \\\n",
       "count  450.000000   450.000000         450.000000  450.000000  450.000000   \n",
       "mean   316.542222   107.162222           3.126667    3.361111    3.468889   \n",
       "std     11.335705     6.023554           1.140254    0.993374    0.919432   \n",
       "min    290.000000    92.000000           1.000000    1.000000    1.000000   \n",
       "25%    308.250000   103.000000           2.000000    2.500000    3.000000   \n",
       "50%    317.000000   107.000000           3.000000    3.500000    3.500000   \n",
       "75%    325.000000   112.000000           4.000000    4.000000    4.000000   \n",
       "max    340.000000   120.000000           5.000000    5.000000    5.000000   \n",
       "\n",
       "             CGPA    Research  Chance of Admit   \n",
       "count  450.000000  450.000000        450.000000  \n",
       "mean     8.577600    0.553333          0.720889  \n",
       "std      0.599454    0.497701          0.141398  \n",
       "min      7.200000    0.000000          0.340000  \n",
       "25%      8.122500    0.000000          0.630000  \n",
       "50%      8.560000    1.000000          0.720000  \n",
       "75%      9.040000    1.000000          0.820000  \n",
       "max      9.920000    1.000000          0.970000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_to_label(df, threshold):\n",
    "    \"\"\"Converts the output feature from continuous values to discrete labels.\"\"\"\n",
    "    df[\"label\"] = [1 if chance >= 0.5 else 0 for chance in df['Chance of Admit ']]\n",
    "    return df.drop(['Chance of Admit '], axis=1, errors='ignore')\n",
    "\n",
    "df = prob_to_label(df_orig, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "      <td>450.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>316.542222</td>\n",
       "      <td>107.162222</td>\n",
       "      <td>3.126667</td>\n",
       "      <td>3.361111</td>\n",
       "      <td>3.468889</td>\n",
       "      <td>8.577600</td>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>11.335705</td>\n",
       "      <td>6.023554</td>\n",
       "      <td>1.140254</td>\n",
       "      <td>0.993374</td>\n",
       "      <td>0.919432</td>\n",
       "      <td>0.599454</td>\n",
       "      <td>0.497701</td>\n",
       "      <td>0.268120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>290.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>308.250000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.122500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>317.000000</td>\n",
       "      <td>107.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>8.560000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>325.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>9.040000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>340.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        GRE Score  TOEFL Score  University Rating         SOP        LOR   \\\n",
       "count  450.000000   450.000000         450.000000  450.000000  450.000000   \n",
       "mean   316.542222   107.162222           3.126667    3.361111    3.468889   \n",
       "std     11.335705     6.023554           1.140254    0.993374    0.919432   \n",
       "min    290.000000    92.000000           1.000000    1.000000    1.000000   \n",
       "25%    308.250000   103.000000           2.000000    2.500000    3.000000   \n",
       "50%    317.000000   107.000000           3.000000    3.500000    3.500000   \n",
       "75%    325.000000   112.000000           4.000000    4.000000    4.000000   \n",
       "max    340.000000   120.000000           5.000000    5.000000    5.000000   \n",
       "\n",
       "             CGPA    Research       label  \n",
       "count  450.000000  450.000000  450.000000  \n",
       "mean     8.577600    0.553333    0.922222  \n",
       "std      0.599454    0.497701    0.268120  \n",
       "min      7.200000    0.000000    0.000000  \n",
       "25%      8.122500    0.000000    1.000000  \n",
       "50%      8.560000    1.000000    1.000000  \n",
       "75%      9.040000    1.000000    1.000000  \n",
       "max      9.920000    1.000000    1.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-1: (10 points) Implement logistic regression model to predict if the student will get admit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionModel:\n",
    "    def __init__(self, learning_rate, number_of_iterations, threshold=0.5):\n",
    "        self.alpha = learning_rate\n",
    "        self.num_iters = number_of_iterations\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n = len(X.columns)  # number of features without x0 column\n",
    "        \n",
    "        # save the mean and std used to normalize so that we can reuse them while predicting\n",
    "        self.mean = X.mean()\n",
    "        self.std = X.std()\n",
    "        \n",
    "        # normalize feature set X\n",
    "        X = self._normalize(X)\n",
    "\n",
    "        # dim(X) = m x n\n",
    "        X = self._prefix_ones_column(X)\n",
    "        \n",
    "        # initialize theta with some values\n",
    "        self.theta = np.zeros((n+1, 1)) ## dim(theta) = n+1 x 1 \n",
    "\n",
    "        # dim(X) = m x n+1\n",
    "        self._gradient_descent(X.values, y.values)\n",
    "        \n",
    "    \n",
    "    def predict_probability(self, X):\n",
    "        X = self._normalize(X)\n",
    "        # dim(X) = m x n\n",
    "        # add a column x0 containing all Ones\n",
    "        X = self._prefix_ones_column(X)    \n",
    "        # dim(X) = m x n+1\n",
    "        # dim(theta) = n+1 x 1\n",
    "        \n",
    "        y_prob = sigmoid(np.dot(X.values, self.theta))\n",
    "        \n",
    "        return y_prob\n",
    "    \n",
    "        \n",
    "    def predict(self, X):\n",
    "        y_prob = self.predict_probability(X)\n",
    "        \n",
    "        y_pred = [1 if prob>=self.threshold else 0 for prob in y_prob]\n",
    "        y_pred_df = pd.DataFrame(data=y_pred, index=X.index)\n",
    "        \n",
    "        return y_pred_df\n",
    "    \n",
    "    \n",
    "    def _gradient_descent(self, X, y):\n",
    "        \"\"\" \n",
    "            theta = n+1 x 1\n",
    "            alpha = scalar\n",
    "            X = m x (n+1)\n",
    "            h = m x 1\n",
    "            y = m x 1\n",
    "            -----------------------------------------\n",
    "            theta = theta - (alpha/m) * X.T * (h - y)\n",
    "        \"\"\"\n",
    "\n",
    "        m = len(X) # number of samples\n",
    "        \n",
    "        for it in range(self.num_iters):\n",
    "            h = sigmoid(np.dot(X, self.theta)) # dim(h) = m x 1\n",
    "            \n",
    "            error = h - y # dim(error) = m x 1\n",
    "            self.theta = self.theta - (self.alpha * np.dot(X.T, error) / m)\n",
    "       \n",
    "    \n",
    "    def _prefix_ones_column(self, df):\n",
    "        # add a column x0 containing all Ones\n",
    "        df = df.assign(x0=pd.Series(np.ones(len(df))).values)\n",
    "\n",
    "        # make x0 the first column\n",
    "        columns = df.columns.tolist()\n",
    "        columns.remove('x0')\n",
    "        df = df[['x0'] + columns]\n",
    "        return df\n",
    "    \n",
    "    def _normalize(self, df):\n",
    "        df = (df - self.mean) / self.std\n",
    "        return df\n",
    "        \n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Sigmoid function :\n",
    "        g(z) = 1 / (1 + e^(-z))\n",
    "        -z can be performed in z is a numpy array \n",
    "        else we need to do: 1 + np.exp(np.negative(z))\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegressionModel(learning_rate=0.01, number_of_iterations=500)\n",
    "\n",
    "\n",
    "X = df.drop([\"label\"], axis=1)\n",
    "y = df[[\"label\"]]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, 0.2,random_state=100) \n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "count  90.0\n",
       "mean    1.0\n",
       "std     0.0\n",
       "min     1.0\n",
       "25%     1.0\n",
       "50%     1.0\n",
       "75%     1.0\n",
       "max     1.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n",
      "Precision: 0.9333333333333333\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score\n",
    "print(\"Accuracy:\", accuracy_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"Precision:\", precision_score(y_pred=y_pred, y_true=y_test))\n",
    "print(\"Recall:\", recall_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-2: (5 points) Compare the performances of logistic regression model with KNN model on the Admission dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Accuracy: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model using the training set\n",
    "knn_model.fit(X_train, y_train.values.reshape(len(y_train),))\n",
    "\n",
    "# Predict Output\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "# Report accuracy\n",
    "print(\"KNN Accuracy:\", accuracy_score(y_pred=y_pred_knn, y_true=y_test))\n",
    "# accuracy_score(y_true=y_test.values.reshape(len(y_test),), y_pred=y_test.values.reshape(len(y_test),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-3: (5 points) \n",
    "- Plot a graph explaining the co-relation between threshold value vs precision and recall.\n",
    "- Report the criteria one should use while deciding the threshold value. \n",
    "- Explain the reason behind your choice of threshold in your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f3346bbf588>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 527.25x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "\n",
    "data = []\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "\n",
    "for threshold in thresholds:\n",
    "    model = LogisticRegressionModel(0.1, 500, threshold=threshold)\n",
    "\n",
    "    df = prob_to_label(df_orig, threshold)    \n",
    "    \n",
    "    X = df.drop([\"label\"], axis=1)\n",
    "    y = df[[\"label\"]]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, 0.2, random_state=1) \n",
    "\n",
    "    model.fit(X_train,y_train)  \n",
    "    y_pred = model.predict(X_test)\n",
    "    precision = precision_score(y_pred=y_pred, y_true=y_test)\n",
    "    recall = recall_score(y_pred=y_pred, y_true=y_test)\n",
    "\n",
    "    data.append((threshold, precision,\"precision\"))\n",
    "    data.append((threshold, recall,\"recall\"))\n",
    "    \n",
    "plotdf = pd.DataFrame(data, index=range(len(data)), columns=[\"threshold\", \"metric\", \"metric_type\"])\n",
    "sns.relplot(x=\"threshold\", y=\"metric\", kind=\"line\", hue=\"metric_type\", data=plotdf, height=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3. (10 points) Implement logistic regression using One vs All and One vs One approaches.\n",
    "Use the following dataset http://preon.iiit.ac.in/~sanjoy_chowdhury/wine-quality.zip for completing the task.  \n",
    "Report your observations and accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.2</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.026</td>\n",
       "      <td>31.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.9916</td>\n",
       "      <td>2.93</td>\n",
       "      <td>0.37</td>\n",
       "      <td>11.3</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            9.2              0.25         0.34             1.2      0.026   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 31.0                  93.0   0.9916  2.93       0.37   \n",
       "\n",
       "   alcohol  quality  \n",
       "0     11.3        7  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig = pd.read_csv(\"data/wine-quality/data.csv\", sep=';',skiprows=1,\n",
    "                 names=[\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\",\"chlorides\",\n",
    "                        \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\", \"pH\", \"sulphates\", \"alcohol\",\n",
    "                        \"quality\"])\n",
    "df_orig.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4408.000000</td>\n",
       "      <td>4408.000000</td>\n",
       "      <td>4408.000000</td>\n",
       "      <td>4408.000000</td>\n",
       "      <td>4408.000000</td>\n",
       "      <td>4408.000000</td>\n",
       "      <td>4408.000000</td>\n",
       "      <td>4408.000000</td>\n",
       "      <td>4408.000000</td>\n",
       "      <td>4408.000000</td>\n",
       "      <td>4408.000000</td>\n",
       "      <td>4408.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6.852189</td>\n",
       "      <td>0.278565</td>\n",
       "      <td>0.333786</td>\n",
       "      <td>6.355377</td>\n",
       "      <td>0.045758</td>\n",
       "      <td>35.307849</td>\n",
       "      <td>138.007827</td>\n",
       "      <td>0.994001</td>\n",
       "      <td>3.187675</td>\n",
       "      <td>0.488185</td>\n",
       "      <td>10.513036</td>\n",
       "      <td>5.873866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.838939</td>\n",
       "      <td>0.100747</td>\n",
       "      <td>0.121491</td>\n",
       "      <td>4.981474</td>\n",
       "      <td>0.022044</td>\n",
       "      <td>17.024667</td>\n",
       "      <td>41.854932</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.150323</td>\n",
       "      <td>0.113913</td>\n",
       "      <td>1.226730</td>\n",
       "      <td>0.882972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.900000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.987110</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.300000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>1.700000</td>\n",
       "      <td>0.036000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>108.000000</td>\n",
       "      <td>0.991720</td>\n",
       "      <td>3.090000</td>\n",
       "      <td>0.410000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.800000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>0.043000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.993700</td>\n",
       "      <td>3.180000</td>\n",
       "      <td>0.470000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.300000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>0.996040</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>0.542500</td>\n",
       "      <td>11.400000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>14.200000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>1.660000</td>\n",
       "      <td>31.600000</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>289.000000</td>\n",
       "      <td>440.000000</td>\n",
       "      <td>1.010300</td>\n",
       "      <td>3.810000</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
       "count    4408.000000       4408.000000  4408.000000     4408.000000   \n",
       "mean        6.852189          0.278565     0.333786        6.355377   \n",
       "std         0.838939          0.100747     0.121491        4.981474   \n",
       "min         3.900000          0.080000     0.000000        0.600000   \n",
       "25%         6.300000          0.210000     0.270000        1.700000   \n",
       "50%         6.800000          0.260000     0.310000        5.200000   \n",
       "75%         7.300000          0.320000     0.390000        9.800000   \n",
       "max        14.200000          1.100000     1.660000       31.600000   \n",
       "\n",
       "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
       "count  4408.000000          4408.000000           4408.000000  4408.000000   \n",
       "mean      0.045758            35.307849            138.007827     0.994001   \n",
       "std       0.022044            17.024667             41.854932     0.002909   \n",
       "min       0.009000             2.000000              9.000000     0.987110   \n",
       "25%       0.036000            23.000000            108.000000     0.991720   \n",
       "50%       0.043000            34.000000            134.000000     0.993700   \n",
       "75%       0.050000            46.000000            167.000000     0.996040   \n",
       "max       0.346000           289.000000            440.000000     1.010300   \n",
       "\n",
       "                pH    sulphates      alcohol      quality  \n",
       "count  4408.000000  4408.000000  4408.000000  4408.000000  \n",
       "mean      3.187675     0.488185    10.513036     5.873866  \n",
       "std       0.150323     0.113913     1.226730     0.882972  \n",
       "min       2.720000     0.220000     8.000000     3.000000  \n",
       "25%       3.090000     0.410000     9.500000     5.000000  \n",
       "50%       3.180000     0.470000    10.400000     6.000000  \n",
       "75%       3.280000     0.542500    11.400000     6.000000  \n",
       "max       3.810000     1.080000    14.200000     9.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_orig.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "quality_scores = np.unique(df_orig[\"quality\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df.drop([\"quality\"], axis=1)\n",
    "# y = df[[\"quality\"]]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, 0.2)\n",
    "# X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-vs-All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary_class(df, true_class):\n",
    "    df[\"label\"] = [1 if quality==true_class else 0 for quality in df['quality']]\n",
    "    return df.drop(['quality'], axis=1, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5147392290249433\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(quality_scores)\n",
    "num_features = len(df_orig.columns) # -1 for label column and +1 for x0 column (column of ones)\n",
    "\n",
    "X = df_orig.drop([\"quality\"], axis=1)\n",
    "y = df_orig[[\"quality\"]]\n",
    "\n",
    "# split to train validate \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, 0.2)\n",
    "\n",
    "# dim(label_probs) = m x num_unique_labels + 1(for highest probability label)\n",
    "label_probs = np.zeros(shape=(len(y_test), num_labels + 1))\n",
    "\n",
    "for i, quality in enumerate(quality_scores):\n",
    "    # convert multiclass to binary class\n",
    "    y_train_binary = convert_to_binary_class(y_train.copy(), quality)\n",
    "\n",
    "    model = LogisticRegressionModel(learning_rate=0.01, number_of_iterations=500)\n",
    "\n",
    "    model.fit(X_train, y_train_binary)\n",
    "    y_prob = model.predict_probability(X_test)\n",
    "    label_probs[:,i] = y_prob.ravel()\n",
    "\n",
    "for row in range(len(label_probs)):\n",
    "    # set label to the one with highest probability\n",
    "    label_probs[row, -1] = quality_scores[label_probs[row].argmax()]\n",
    "\n",
    "\n",
    "y_pred = label_probs[:,-1]\n",
    "print(accuracy_score(y_pred=y_pred, y_true=y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-vs-One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
